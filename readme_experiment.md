# WAN Video Generator — Train and inference

Эта ветка содержит код для обучения и инференса модели генерации видео на основе текста и/или изображения. Проект включает скрипты для обучения (`train.py`) и инференса (`inference.py`), а также Jupyter-ноутбук для экспериментов.

## Структура проекта

```
.
├── notebooks
│   └── experiments.ipynb  # Ноутбук для экспериментов и прототипирования
├── readme_experiment.md
├── requirements.txt       # Список зависимостей
├── src
│   ├── inference.py       # Скрипт для генерации видео (инференс)
│   └── train.py           # Скрипт для обучения модели
```

## Установка

Для запуска кода установите необходимые зависимости из файла `requirements.txt`:

```
pip install -r requirements.txt
```

Убедитесь, что у вас установлена подходящая версия Python (рекомендуется Python 3.11+). Также требуется наличие CUDA для работы с GPU.

## Модель

В качестве базовой модели выбрана **wan2.2-ti2v-5b**. Она обеспечивает приемлемое качество генерации видео и может запускаться на потребительских видеокартах. Модель подходит для задач text-to-video (T2V) и image-to-video (I2V) generation.

## Требования к оборудованию

- **Инференс**: Минимально рекомендуется использовать видеокарту NVIDIA RTX 3090 с 24 ГБ VRAM. Время генерации 5-секундного видео (121 кадров) составляет примерно 4 минуты.
- **Обучение**: Для обучения модели требуется более мощное оборудование, такое как NVIDIA A100 или NVIDIA H100 с 80 ГБ VRAM.

