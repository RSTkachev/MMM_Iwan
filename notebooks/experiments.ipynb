{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoencoderKLWan, WanImageToVideoPipeline\n",
    "from diffusers.utils import export_to_video, load_image\n",
    "\n",
    "if torch.cuda.device_count() != 1:\n",
    "    raise Exception(\"Not correct number of GPUs\")\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Wan-AI/Wan2.2-TI2V-5B-Diffusers\"\n",
    "\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32, cache_dir=\"./weights\")\n",
    "pipe = WanImageToVideoPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16, cache_dir=\"./weights\")\n",
    "pipe.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\",\n",
    ")\n",
    "max_area = 1280 * 704\n",
    "aspect_ratio = image.height / image.width\n",
    "mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n",
    "height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n",
    "width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n",
    "image = image.resize((width, height))\n",
    "prompt = (\n",
    "    \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in \"\n",
    "    \"the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n",
    ")\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "\n",
    "output = pipe(\n",
    "    image=image,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    height=height, width=width,\n",
    "    num_frames=121,\n",
    ").frames[0]\n",
    "export_to_video(output, \"output.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('i2v_input.JPG')\n",
    "prompt = \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./weights_finetuned\"\n",
    "lora_rank = 32\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "batch_size = 1\n",
    "accumulation_steps = 4\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "dataset_file = \"train.jsonl\"\n",
    "num_frames = 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"bf16\", gradient_accumulation_steps=accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = pipe.vae\n",
    "text_encoder = pipe.text_encoder\n",
    "scheduler = pipe.scheduler\n",
    "image_processor = pipe.image_processor\n",
    "dit = pipe.transformer\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\n",
    "        \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",\n",
    "        \"ffn.net.0.proj\", \"ffn.net.2\",\n",
    "        \"proj_out\"\n",
    "    ],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dit = get_peft_model(dit, lora_config)\n",
    "dit.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(dit.parameters(), lr=learning_rate)\n",
    "dit, optimizer = accelerator.prepare(dit, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    global pipe\n",
    "    global image_processor\n",
    "    global transform\n",
    "    global vae\n",
    "    global text_encoder\n",
    "    \n",
    "    img = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    img = img.resize((1280, 704))\n",
    "    \n",
    "    cap = cv2.VideoCapture(example[\"video\"])\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame).resize((1280, 704))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if len(frames) != num_frames:\n",
    "        raise ValueError(f\"Video must have {num_frames} frames\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if image_processor is not None:\n",
    "            img_input = image_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        else:\n",
    "            img_input = transform(img).unsqueeze(0).to(device)\n",
    "        img_input = img_input.unsqueeze(2)\n",
    "\n",
    "        if image_processor is not None:\n",
    "            frame_tensors = [image_processor(f, return_tensors=\"pt\").pixel_values.squeeze(0).to(device) for f in frames]\n",
    "        else:\n",
    "            frame_tensors = [transform(f).to(device) for f in frames]\n",
    "        video_tensor = torch.stack(frame_tensors, dim=0).to(device)\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "\n",
    "        if hasattr(pipe, 'image_encoder') and pipe.image_encoder is not None:\n",
    "            img_latent = pipe.image_encoder(img_input).last_hidden_state.squeeze(0)\n",
    "        else:\n",
    "            img_latent = vae.encode(img_input).latent_dist.sample().squeeze(0)\n",
    "\n",
    "        video_latents = vae.encode(video_tensor).latent_dist.sample().squeeze(0)\n",
    "\n",
    "        if img_latent.dim() == 4 and img_latent.shape[1] == 1:\n",
    "            img_latent = img_latent.squeeze(1)\n",
    "\n",
    "    if \"prompt\" in example:\n",
    "        tokens = pipe.tokenizer(example[\"prompt\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "        text_emb = text_encoder(tokens).last_hidden_state\n",
    "    else:\n",
    "        text_emb = None\n",
    "\n",
    "    return {\"img_latent\": img_latent, \"text_emb\": text_emb, \"video_latents\": video_latents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    collated = {}\n",
    "    for key in batch[0].keys():\n",
    "        values = [item[key] for item in batch]\n",
    "        if key == \"text_emb\":\n",
    "            non_none_values = [v for v in values if v is not None]\n",
    "            if non_none_values:\n",
    "                max_seq = max(v.shape[1] for v in non_none_values)\n",
    "                padded = []\n",
    "                for v in values:\n",
    "                    if v is None:\n",
    "                        zero_shape = list(non_none_values[0].shape)\n",
    "                        zero_shape[1] = max_seq\n",
    "                        padded.append(torch.zeros(zero_shape, dtype=non_none_values[0].dtype, device=non_none_values[0].device))\n",
    "                    else:\n",
    "                        pad_amount = (0, 0, 0, max_seq - v.shape[1])\n",
    "                        padded.append(torch.nn.functional.pad(v, pad_amount))\n",
    "                collated[key] = torch.cat(padded, dim=0)\n",
    "            else:\n",
    "                collated[key] = None\n",
    "        else:\n",
    "            collated[key] = torch.stack(values)\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
    "processed_dataset = []\n",
    "for sample in tqdm(dataset):\n",
    "    processed_dataset.append(preprocess(sample))\n",
    "dataloader = DataLoader(processed_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dit.train()\n",
    "for epoch in range(epochs):\n",
    "    progress = tqdm(dataloader)\n",
    "    for step, batch in enumerate(progress):\n",
    "        with accelerator.accumulate(dit):\n",
    "            latents = batch[\"video_latents\"]\n",
    "            img_latent = batch[\"img_latent\"]\n",
    "            text_emb = batch[\"text_emb\"]\n",
    "\n",
    "            cond = text_emb\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch_size,), device=latents.device)\n",
    "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            img_latent = img_latent.unsqueeze(2)\n",
    "            img_noise = noise[:, :, :1, :, :]\n",
    "            noisy_img = scheduler.add_noise(img_latent, img_noise, timesteps)\n",
    "            noisy_latents[:, :, :1, :, :] = noisy_img\n",
    "\n",
    "            model_pred = dit(noisy_latents, timesteps, encoder_hidden_states=cond)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    accelerator.save_state(os.path.join(output_dir, f\"checkpoint_{epoch}\"))\n",
    "\n",
    "dit.save_pretrained(os.path.join(output_dir, \"lora_adapter_transformer\"))\n",
    "print(\"Fine-tuning завершён. LoRA сохранён в\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "notebookId": "d939206a-bf6c-4507-b691-b1019e21de8e",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
